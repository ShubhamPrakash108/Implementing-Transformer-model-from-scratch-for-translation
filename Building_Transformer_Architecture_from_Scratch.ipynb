{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 915334,
          "sourceType": "datasetVersion",
          "datasetId": 492138
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:24.168918Z",
          "iopub.execute_input": "2024-11-25T12:28:24.169722Z",
          "iopub.status.idle": "2024-11-25T12:28:27.276534Z",
          "shell.execute_reply.started": "2024-11-25T12:28:24.169676Z",
          "shell.execute_reply": "2024-11-25T12:28:27.275486Z"
        },
        "id": "Yr6egwX3A5bz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a class for input embeddings with scaling based on the model's dimensionality.\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:36.773382Z",
          "iopub.execute_input": "2024-11-25T12:28:36.774214Z",
          "iopub.status.idle": "2024-11-25T12:28:36.779705Z",
          "shell.execute_reply.started": "2024-11-25T12:28:36.774159Z",
          "shell.execute_reply": "2024-11-25T12:28:36.778518Z"
        },
        "id": "Geel7SdhA5bz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines classes for positional encoding with sinusoidal functions and a residual connection with layer normalization.\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.359949Z",
          "iopub.execute_input": "2024-11-25T12:28:37.360741Z",
          "iopub.status.idle": "2024-11-25T12:28:37.370458Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.360706Z",
          "shell.execute_reply": "2024-11-25T12:28:37.369431Z"
        },
        "id": "HSVTrgoLA5b0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements layer normalization to stabilize and accelerate training by normalizing the input across features.\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim = True)\n",
        "        std = x.std(dim = -1, keepdim = True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.386369Z",
          "iopub.execute_input": "2024-11-25T12:28:37.386645Z",
          "iopub.status.idle": "2024-11-25T12:28:37.392765Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.386619Z",
          "shell.execute_reply": "2024-11-25T12:28:37.391734Z"
        },
        "id": "iu9nEOVzA5b0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a feedforward neural network block with two linear layers and dropout for regularization.\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.443462Z",
          "iopub.execute_input": "2024-11-25T12:28:37.444277Z",
          "iopub.status.idle": "2024-11-25T12:28:37.44921Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.444244Z",
          "shell.execute_reply": "2024-11-25T12:28:37.448255Z"
        },
        "id": "bUokE2zvA5b0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements a multi-head attention block with query, key, and value transformations, and scaled dot-product attention.\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.462142Z",
          "iopub.execute_input": "2024-11-25T12:28:37.462472Z",
          "iopub.status.idle": "2024-11-25T12:28:37.472859Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.462443Z",
          "shell.execute_reply": "2024-11-25T12:28:37.472004Z"
        },
        "id": "wdHDL5bdA5b0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a residual connection with layer normalization and dropout for improved gradient flow.\n",
        "class ResidualConnection(nn.Module):\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.506413Z",
          "iopub.execute_input": "2024-11-25T12:28:37.506685Z",
          "iopub.status.idle": "2024-11-25T12:28:37.511904Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.506658Z",
          "shell.execute_reply": "2024-11-25T12:28:37.51108Z"
        },
        "id": "p4DKWxzKA5b0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements an encoder block with self-attention, feedforward layers, and residual connections, as well as an encoder composed of multiple layers.\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.519679Z",
          "iopub.execute_input": "2024-11-25T12:28:37.520248Z",
          "iopub.status.idle": "2024-11-25T12:28:37.526837Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.520218Z",
          "shell.execute_reply": "2024-11-25T12:28:37.526008Z"
        },
        "id": "smRiZa7xA5b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements a decoder block with self-attention, cross-attention, feedforward layers, and residual connections, as well as a decoder composed of multiple layers.\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.58593Z",
          "iopub.execute_input": "2024-11-25T12:28:37.586232Z",
          "iopub.status.idle": "2024-11-25T12:28:37.593993Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.586205Z",
          "shell.execute_reply": "2024-11-25T12:28:37.593029Z"
        },
        "id": "vxGRySpZA5b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a projection layer that maps the model's output to the vocabulary size for each sequence element.\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x) -> None:\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.596454Z",
          "iopub.execute_input": "2024-11-25T12:28:37.59676Z",
          "iopub.status.idle": "2024-11-25T12:28:37.606556Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.596733Z",
          "shell.execute_reply": "2024-11-25T12:28:37.605842Z"
        },
        "id": "t68LDK3oA5b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "    return mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-26T10:09:51.286472Z",
          "iopub.execute_input": "2024-11-26T10:09:51.286806Z",
          "iopub.status.idle": "2024-11-26T10:09:51.296024Z",
          "shell.execute_reply.started": "2024-11-26T10:09:51.286767Z",
          "shell.execute_reply": "2024-11-26T10:09:51.29516Z"
        },
        "id": "Bj9q15kOA5b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a Transformer model with encoding, decoding, and projection layers for sequence-to-sequence tasks.\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.647933Z",
          "iopub.execute_input": "2024-11-25T12:28:37.64824Z",
          "iopub.status.idle": "2024-11-25T12:28:37.655109Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.648212Z",
          "shell.execute_reply": "2024-11-25T12:28:37.654183Z"
        },
        "id": "Ik9FfpagA5b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Builds a Transformer model with specified hyperparameters, including encoder and decoder blocks, embedding layers, and positional encodings.\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:37.659902Z",
          "iopub.execute_input": "2024-11-25T12:28:37.660215Z",
          "iopub.status.idle": "2024-11-25T12:28:37.669563Z",
          "shell.execute_reply.started": "2024-11-25T12:28:37.660186Z",
          "shell.execute_reply": "2024-11-25T12:28:37.668505Z"
        },
        "id": "uyzE3a_9A5b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:38.686934Z",
          "iopub.execute_input": "2024-11-25T12:28:38.687276Z",
          "iopub.status.idle": "2024-11-25T12:28:39.140635Z",
          "shell.execute_reply.started": "2024-11-25T12:28:38.687245Z",
          "shell.execute_reply": "2024-11-25T12:28:39.139823Z"
        },
        "id": "K-rX2ZeDA5b2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "csv = pd.read_csv(\"/kaggle/input/english-to-hindi-parallel-dataset/newdata.csv\")\n",
        "csv.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:39.142165Z",
          "iopub.execute_input": "2024-11-25T12:28:39.142558Z",
          "iopub.status.idle": "2024-11-25T12:28:40.980906Z",
          "shell.execute_reply.started": "2024-11-25T12:28:39.142529Z",
          "shell.execute_reply": "2024-11-25T12:28:40.980072Z"
        },
        "id": "G3U9-lKWA5b2",
        "outputId": "3193e213-a5d7-4e25-8ab0-0e9a265f58a4"
      },
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0                                   english_sentence  \\\n0           0  politicians do not have permission to do what ...   \n1           1         I'd like to tell you about one such child,   \n2           2  This percentage is even greater than the perce...   \n3           3  what we really mean is that they're bad at not...   \n4           4  .The ending portion of these Vedas is called U...   \n\n                                      hindi_sentence  \n0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>I'd like to tell you about one such child,</td>\n      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>This percentage is even greater than the perce...</td>\n      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>what we really mean is that they're bad at not...</td>\n      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>.The ending portion of these Vedas is called U...</td>\n      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "csv.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "csv.dropna()\n",
        "csv = csv.dropna()\n",
        "csv.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:28:40.983008Z",
          "iopub.execute_input": "2024-11-25T12:28:40.983793Z",
          "iopub.status.idle": "2024-11-25T12:28:41.098966Z",
          "shell.execute_reply.started": "2024-11-25T12:28:40.983746Z",
          "shell.execute_reply": "2024-11-25T12:28:41.097914Z"
        },
        "id": "G3kca4dcA5b2",
        "outputId": "f6807c49-e06a-4cfd-9565-056df2b869b9"
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                    english_sentence  \\\n0  politicians do not have permission to do what ...   \n1         I'd like to tell you about one such child,   \n2  This percentage is even greater than the perce...   \n3  what we really mean is that they're bad at not...   \n4  .The ending portion of these Vedas is called U...   \n\n                                      hindi_sentence  \n0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'd like to tell you about one such child,</td>\n      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This percentage is even greater than the perce...</td>\n      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what we really mean is that they're bad at not...</td>\n      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.The ending portion of these Vedas is called U...</td>\n      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from tokenizers import Tokenizer, trainers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:29:38.360856Z",
          "iopub.execute_input": "2024-11-25T12:29:38.36175Z",
          "iopub.status.idle": "2024-11-25T12:29:38.366886Z",
          "shell.execute_reply.started": "2024-11-25T12:29:38.361711Z",
          "shell.execute_reply": "2024-11-25T12:29:38.365786Z"
        },
        "id": "6iJtXmchA5b3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Processes the dataset by reading, tokenizing, splitting into training and validation sets, and creating data loaders.\n",
        "def process_data(config):\n",
        "    df = pd.read_csv(config[\"data_path\"]).dropna().reset_index(drop=True)\n",
        "    tokenizer_src = build_tokenizer(df[\"english_sentence\"], config[\"tokenizer_file\"].format(\"en\"))\n",
        "    tokenizer_tgt = build_tokenizer(df[\"hindi_sentence\"], config[\"tokenizer_file\"].format(\"hi\"))\n",
        "\n",
        "    train_size = int(len(df) * 0.9)\n",
        "    train_df, val_df = df.iloc[:train_size], df.iloc[train_size:]\n",
        "\n",
        "    train_ds = BilingualDataset(train_df, tokenizer_src, tokenizer_tgt, config[\"seq_len\"])\n",
        "    val_ds = BilingualDataset(val_df, tokenizer_src, tokenizer_tgt, config[\"seq_len\"])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=1)\n",
        "\n",
        "    return train_loader, val_loader, tokenizer_src, tokenizer_tgt\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:29:39.354012Z",
          "iopub.execute_input": "2024-11-25T12:29:39.35438Z",
          "iopub.status.idle": "2024-11-25T12:29:39.361271Z",
          "shell.execute_reply.started": "2024-11-25T12:29:39.354349Z",
          "shell.execute_reply": "2024-11-25T12:29:39.360267Z"
        },
        "id": "ylm-UoQxA5b4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# build_tokenizer creates or loads a tokenizer, while BilingualDataset tokenizes and prepares bilingual text data with special tokens and masks for training.\n",
        "def build_tokenizer(texts, tokenizer_path):\n",
        "    if Path(tokenizer_path).exists():\n",
        "        return Tokenizer.from_file(tokenizer_path)\n",
        "\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = trainers.WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
        "    tokenizer.train_from_iterator(texts, trainer)\n",
        "    tokenizer.save(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer_src, tokenizer_tgt, seq_len):\n",
        "        self.df = df\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.sos_token = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
        "        self.eos_token = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
        "        self.pad_token = tokenizer_tgt.token_to_id(\"[PAD]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.df.iloc[idx][\"english_sentence\"]\n",
        "        tgt_text = self.df.iloc[idx][\"hindi_sentence\"]\n",
        "        enc_input = [self.sos_token] + self.tokenizer_src.encode(src_text).ids + [self.eos_token]\n",
        "        dec_input = [self.sos_token] + self.tokenizer_tgt.encode(tgt_text).ids\n",
        "        label = self.tokenizer_tgt.encode(tgt_text).ids + [self.eos_token]\n",
        "        enc_input = (enc_input + [self.pad_token] * self.seq_len)[:self.seq_len]\n",
        "        dec_input = (dec_input + [self.pad_token] * self.seq_len)[:self.seq_len]\n",
        "        label = (label + [self.pad_token] * self.seq_len)[:self.seq_len]\n",
        "        encoder_mask = torch.tensor(enc_input) != self.pad_token\n",
        "        decoder_mask = torch.tensor(dec_input) != self.pad_token\n",
        "        return {\n",
        "            \"encoder_input\": torch.tensor(enc_input),\n",
        "            \"decoder_input\": torch.tensor(dec_input),\n",
        "            \"label\": torch.tensor(label),\n",
        "            \"encoder_mask\": encoder_mask.unsqueeze(0).unsqueeze(0).int(),\n",
        "            \"decoder_mask\": decoder_mask.unsqueeze(0).unsqueeze(0).int(),\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T12:29:40.02432Z",
          "iopub.execute_input": "2024-11-25T12:29:40.024946Z",
          "iopub.status.idle": "2024-11-25T12:29:40.03596Z",
          "shell.execute_reply.started": "2024-11-25T12:29:40.024911Z",
          "shell.execute_reply": "2024-11-25T12:29:40.034962Z"
        },
        "id": "efdT72aCA5b4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration dictionary\n",
        "def main():\n",
        "    config = {\n",
        "        'batch_size': 8,\n",
        "        'num_epochs': 1,\n",
        "        'lr': 1e-4,\n",
        "        'seq_len': 350,\n",
        "        'd_model': 512,\n",
        "        'data_path': \"/kaggle/input/english-to-hindi-parallel-dataset/newdata.csv\",\n",
        "        'model_folder': 'weights',\n",
        "        'model_basename': 'en_hi_transformer_',\n",
        "        'preload': None,\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'experiment_name': 'runs/en_hi_transformer'\n",
        "    }\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vxx7EhHdA5b4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model directory if it doesn't exist\n",
        "Path(config['model_folder']).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "u3kru9oMA5b4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data and get DataLoader objects\n",
        "print(\"Processing data...\")\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = process_data(config)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "rLY_XV11A5b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tacArIjdA5b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Build transformer model\n",
        "model = build_transformer(\n",
        "        src_vocab_size=tokenizer_src.get_vocab_size(),\n",
        "        tgt_vocab_size=tokenizer_tgt.get_vocab_size(),\n",
        "        src_seq_len=config['seq_len'],\n",
        "        tgt_seq_len=config['seq_len'],\n",
        "        d_model=config['d_model']).to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "u4B96B1mA5b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "loss_fn = nn.CrossEntropyLoss(\n",
        "        ignore_index=tokenizer_src.token_to_id('[PAD]'),\n",
        "        label_smoothing=0.1).to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "csJoP3bsA5b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard writer\n",
        "writer = SummaryWriter(config['experiment_name'])\n",
        "    print(\"Starting training...\")\n",
        "    global_step = 0\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in batch_iterator:\n",
        "        encoder_input = batch['encoder_input'].to(device)\n",
        "        decoder_input = batch['decoder_input'].to(device)\n",
        "        encoder_mask = batch['encoder_mask'].to(device)\n",
        "        decoder_mask = batch['decoder_mask'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "        decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "        proj_output = model.project(decoder_output)\n",
        "        loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        batch_iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        writer.add_scalar('train_loss', loss.item(), global_step)\n",
        "        global_step += 1\n",
        "\n",
        "    print(f\"Epoch {epoch} Loss: {total_loss / len(train_dataloader):.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Rmz70KWYA5b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model checkpoint\n",
        "checkpoint_path = f\"{config['model_folder']}/{config['model_basename']}{epoch:02d}.pt\"\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "if epoch % 5 == 0:\n",
        "    validate(model, val_dataloader, tokenizer_src, tokenizer_tgt, config, device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vQ5-DVFLA5b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate an input sentence using a trained model\n",
        "def translate_sentence(model, tokenizer_src, tokenizer_tgt, sentence, seq_len, device):\n",
        "    model.eval()\n",
        "    input_tokens = [tokenizer_src.token_to_id(\"[SOS]\")] + tokenizer_src.encode(sentence).ids + [tokenizer_src.token_to_id(\"[EOS]\")]\n",
        "    input_tokens = (input_tokens + [tokenizer_src.token_to_id(\"[PAD]\")] * seq_len)[:seq_len]\n",
        "    input_tensor = torch.tensor([input_tokens], device=device)\n",
        "    encoder_mask = (input_tensor != tokenizer_src.token_to_id(\"[PAD]\")).unsqueeze(1).unsqueeze(1).int()\n",
        "    with torch.no_grad():\n",
        "        encoder_output = model.encode(input_tensor, encoder_mask)\n",
        "    decoder_input = torch.tensor([[tokenizer_tgt.token_to_id(\"[SOS]\")]], device=device)\n",
        "    output_tokens = []\n",
        "\n",
        "    for _ in range(seq_len):\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "        next_token = torch.argmax(proj_output[:, -1, :], dim=-1).item()\n",
        "        if next_token == tokenizer_tgt.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "        output_tokens.append(next_token)\n",
        "        decoder_input = torch.cat([decoder_input, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "    translated_sentence = tokenizer_tgt.decode(output_tokens)\n",
        "    return translated_sentence\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-25T14:40:07.680461Z",
          "iopub.execute_input": "2024-11-25T14:40:07.681186Z",
          "iopub.status.idle": "2024-11-25T14:40:09.557864Z",
          "shell.execute_reply.started": "2024-11-25T14:40:07.681152Z",
          "shell.execute_reply": "2024-11-25T14:40:09.556974Z"
        },
        "id": "fMQKq367A5b9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# The code loads pre-trained English and Hindi tokenizers, initializes and loads a transformer model, and translates a list of sentences from English to Hindi using the model.\n",
        "tokenizer_src = Tokenizer.from_file(\"/kaggle/working/tokenizer_en.json\")\n",
        "tokenizer_tgt = Tokenizer.from_file(\"/kaggle/working/tokenizer_hi.json\")\n",
        "\n",
        "model = build_transformer(\n",
        "    src_vocab_size=tokenizer_src.get_vocab_size(),\n",
        "    tgt_vocab_size=tokenizer_tgt.get_vocab_size(),\n",
        "    src_seq_len=350,\n",
        "    tgt_seq_len=350,\n",
        "    d_model=512,\n",
        "    N=6,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    d_ff=2048\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"/kaggle/working/weights/en_hi_transformer_00.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "sentences = [\n",
        "    \"How are you?\",\n",
        "    \"What is your name?\",\n",
        "    \"Politicians do not have permission to do what needs to be done.\",\n",
        "    \"I love machine learning.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    translated_text = translate_sentence(model, tokenizer_src, tokenizer_tgt, sentence, seq_len=350, device=device)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Translated: {translated_text}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UvWMJ6pcA5b9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1oSwQhEA9X2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}